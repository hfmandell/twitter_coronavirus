# Coronavirus Twitter Analysis

**Summary:** Use MapReduce to scan all geotagged tweets sent in 2020 to monitor for the spread of the coronavirus on social media.

## Accomplishments:
1. work with large scale datasets
1. work with multilingual text
1. use the MapReduce divide-and-conquer paradigm to create parallel code

## Findings
By country, in 2020 the US had the most geotagged tweets including "#coronavirus", 
followed by India and Great Britain.
For the word in Korean, "#코로나바이러스", it was most tweeted by The Republic of Korea,
followed by Hong Kong and Spain. However, it's usage paled in comparison to that of the
English word, with the US's "#coronavirus" usage being more than 800 times Korea's 
"#코로나바이러" usage.

<img src=plots/country_#coronavirus_barchart.png width=50% />

<img src=plots/country_#코로나바이러스_barchart.png width=50% />

For language, in 2020 English (en) and Spanish (es) were the top two most frequent 
languages to contain "#coronavirus". Undetermined (und) languages were third.
In Korean, "#코로나바이러스" was most tweeted in Korean (ko), with English (en) 
coming in second and Undetermined (und) languages again being third.

<img src=plots/language_#coronavirus_barchart.png width=50% />

<img src=plots/language_#코로나바이러스_barchart.png width=50% />

## Background

Approximately 500 million tweets are sent everyday.
Of those tweets, about 1% are *geotagged*.
Our big dataset contains all geotagged tweets that were sent in 2020 (1.1 billion tweets in this dataset).

The tweets for each day are stored in a zip file `geoTwitterYY-MM-DD.zip`,
and inside this zip file are 24 text files, one for each hour of the day.
Each text file contains a single tweet per line in JSON format.

We follow the [MapReduce](https://en.wikipedia.org/wiki/MapReduce) procedure to analyze these tweets.
MapReduce is a famous procedure for large scale parallel processing that is widely used in industry.
It is a 3 step procedure summarized in the following image:

<img src=mapreduce.png width=100% />

The partition step is already done (by splitting up the tweets into one file per day).
Next, we have to do the map and reduce steps.

**Runtime:**

The simplest and most common scenario is that the map procedure takes time O(n) and the reduce procedure takes time O(1).
If you have p<<n processors, then the overall runtime will be O(n/p).
This means that:
1. doubling the amount of data will cause the analysis to take twice as long;
1. doubling the number of processors will cause the analysis to take half as long;
1. if you want to add more data and keep the processing time the same, then you need to add a proportional number of processors.

1. **Mapping:**
   The `map.py` file processes a single zip file of tweets.

1. **Visualizing:** 
   The `visualize.py` file displays the output from running the `map.py` file.

1. **Reducing:**
   The `reduce.py` file merges the outputs generated by the `map.py` file so that the combined files can be visualized.
   Generate a new output file by running the command

## What was completed?

1. We can now track the usage of hashtags on both a language and country level.
   This is done by creating variables `counter_country` and `counter_lang` in `src/map.py`.
   The output of running `map.py` is now two files, one that ends in `.lang` for the lanuage dictionary,
   and one that ends in `.country` for the country dictionary.
   Most tweets contain a `place` key, which contains a dictionary with the `country_code` key.
   This is how we lookup the country that a tweet was sent from.

1. We created a shell script `run_maps.sh` to loop over each file in the dataset and run `map.py` on it in order to track results for each country.
   Since each call to `map.py` can take between minutes to hours to finish,
   we use the `nohup` command to ensure the program continues to run after you disconnect and the `&` operator to ensure that all `map.py` commands run in parallel.

1. After this `map.py` has run on all the files,
   we have a large number of files in the `outputs` folder.
   We use `reduce.py` to combine all of the `.lang` files into a single file,
   and all of the `.country` files into a different file.
   Then, we use `visualize.py` to displauy the total number of occurrences of each of the hashtags on a bar chart`.
